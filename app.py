import streamlit as st
import logging
from utils import MEMORY, DocumentLoader
from chat import config_retrieval_chain
from streamlit.external.langchain import StreamlitCallbackHandler


logging.basicConfig(encoding="utf-8", level=logging.INFO)
LOGGER = logging.getLogger()

st.set_page_config(page_title="æ–‡ä»¶", page_icon="ğŸ‘»")
st.title("ğŸ‘»æ–‡ä»¶å•ç­”ğŸ‘»")

uploaded_files = st.sidebar.file_uploader(
    label="ä¸Šå‚³æ–‡ä»¶ğŸ£",
    type=list(DocumentLoader.supported_extensions.keys()),
    accept_multiple_files=True
)
if not uploaded_files:
    st.info("è«‹ä¸Šå‚³é™„ä»¶ğŸ£")
    st.stop()

use_chunk = st.sidebar.slider(
    'æ–‡ç« åˆ‡å‰²',
    500, 2000, (1000)
)
use_temperature = st.sidebar.slider(
     'æº«åº¦ (è¶Šé«˜è¶Šæœ‰å‰µæ„ğŸ¦„)',
     0.0, 1.0, (0.1))

use_compression = st.checkbox("CompressionğŸ› ï¸(ä¸Šå‚³æ–‡ä»¶)", value=False)
use_moderation = st.checkbox("ModerationğŸ•¸ï¸(éæ¿¾ä¸åˆå®œå›ç­”)", value=False)
use_ddg_search = st.checkbox("ä½¿ç”¨DuckDuckGOæœå°‹ğŸ¦†(ä¸æœƒä½¿ç”¨ä¸Šå‚³æ–‡ä»¶)", value=False)

CONV_CHAIN = config_retrieval_chain(
    uploaded_files,
    use_compression=use_compression,
    use_moderation=use_moderation,
    use_chunksize=use_chunk,
    use_temperature=use_temperature,
    use_zeroshoot=use_ddg_search
)

if st.sidebar.button("æ¸…é™¤å°è©±å’Œå å­˜å°è©±ğŸ¦­"):
    MEMORY.chat_memory.clear()

avatars = {"human": "user", "ai": "assistant"}

if len(MEMORY.chat_memory.messages) == 0:
    st.chat_message("assistant").markdown("è«‹æå•ğŸ¤–")

assistant = st.chat_message("assistant")
if user_query := st.chat_input(placeholder="èªªé»ä»€éº¼"):
    st.chat_message("user").write(user_query)
    container = st.empty()
    stream_handler = StreamlitCallbackHandler(container)
    with st.chat_message("assistant"):
        if use_ddg_search:
            response = CONV_CHAIN.invoke(
                {"input": user_query}, {"callbacks": [stream_handler]}
            )
            st.write(response["output"])
        else:
            params = {
                "question": user_query,
                "chat_history": MEMORY.chat_memory.messages,
            }
            response = CONV_CHAIN.run(params, callbacks=[stream_handler])
            if response:
                container.markdown(response)